<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Dengyang Jiang</title>
    <meta name="author" content="Dengyang Jiang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/logo.jpg" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <style>
      body {
        background-color: #ffffff; /* White background for better contrast */
        color: #333; /* Default text color to ensure readability */
      }
      .section {
        padding: 2.5%;
        width: 100%;
      }
      .title {
        font-size: 1.6em;
        color: #333;
        margin-bottom: 20px;
      }
      .subsection {
        margin-bottom: 25px;
      }
      .sub-name {
        font-size: 1.3em;
        font-weight: bold;
        margin-bottom: 5px;
      }
      .content {
        font-size: 1em;
        font-weight: 600;
        margin-bottom: 5px;
      }
      .author {
        font-size: 1em;
        font-weight: 400;
        margin-bottom: 5px;
      }

      /* Add these new styles */
      .institution-wrapper {
        display: flex;
        align-items: flex-start;
        gap: 20px;
        margin-bottom: 30px;
      }

      .institution-logo {
        width: 60px;
        height: 60px;
        object-fit: contain;
      }

      .institution-content {
        flex: 1;
      }

      /* For mobile responsiveness */
      @media (max-width: 600px) {
        .institution-wrapper {
          gap: 15px;
        }
        .institution-logo {
          width: 45px;
          height: 45px;
        }
      }
</style>
  </head>
  <!-- Top Main Section -->
 <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  <img src="images/logo.jpg" alt="Logo" style="height: 2em; vertical-align: middle; margin-right: 5px;">
                    Dengyang Jiang
                </p>
                <p style="font-size: larger;">
                  Hi, there! I'm Dengyang Jiang, currently a Ph.D. candidate at The Hong Kong University of Science and Technology, supervised by Prof. <a style="font-size: inherit;" href="http://hyang.org">Harry Yang</a>. 
                </p>
                <p style="font-size: larger;">
                   My research interests encompass deep learning and computer vision. I am delighted to communicate and collaborate with anyone interested in this field. 
                  Feel free to contact me via <a style="font-size: inherit;" href="mailto:vvvvvjdy@gmail.com">Email</a> or <a style="font-size: inherit;" href="https://www.xiaohongshu.com/user/profile/60195f8f0000000001009cc6?tab=note">Xiaohongshu(RedNote)</a>.
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/me.jpg"><img style="width:100%;max-width:100%;object-fit: cover;" alt="profile photo" src="images/me.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>

            
<!-- Education Section -->
<tr>
  <td colspan="2" class="section">
    <h2 class="title">Education Experience</h2>
    <div class="subsection">
      <div class="institution-wrapper">
        <img src="images/institution/nwpu.jpg" alt="NWPU Logo" class="institution-logo">
        <div class="institution-content">
          <div class="sub-name">School of Automation, Northwestern Polytechnical University, 2022-2026</div>
          <div class="content">B.S.E., supervised by Prof. <a href="https://teacher.nwpu.edu.cn/nwpuzhanglei.html">Lei Zhang</a>, in the research team leaded by Prof. <a href="https://teacher.nwpu.edu.cn/ynzhang">Yanning Zhang</a>. </div>
        </div>
      </div>
    </div>
  </td>
</tr>

<!-- Work Section -->
<tr>
  <td colspan="2" class="section">
    <h2 class="title">Work Experience</h2>

     <div class="subsection">
      <div class="institution-wrapper">
        <img src="images/institution/alibaba.jpg" alt="Ali Logo" class="institution-logo">
        <div class="institution-content">
          <div class="sub-name">Tongyi Lab, Alibaba Group, 2025.10-present</div>
          <div class="content">Research Intern, mentored by Dr. <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=_go6DPsAAAAJ">Peng Gao</a>,
            also work with Prof. <a href="https://sites.google.com/view/stevenhoi/home">Steven Hoi</a>.</div>
        </div>
      </div>
    </div>

    <div class="subsection">
      <div class="institution-wrapper">
        <img src="images/institution/shailab.jpg" alt="Ali Logo" class="institution-logo">
        <div class="institution-content">
          <div class="sub-name">Shanghai Artificial Intelligence Laboratory, 2025.07-2025.10</div>
          <div class="content">Research Intern, mentored by Dr. <a href="https://bobrown.github.io/boZhang.github.io/">Bo Zhang</a>,
           also work with Dr. <a href="https://scholar.google.com/citations?user=miFIAFMAAAAJ&hl=en">Peng Gao</a>.</div>
        </div>
      </div>
    </div> 
    
    <div class="subsection">
      <div class="institution-wrapper">
        <img src="images/institution/sg.jpg" alt="SG Logo" class="institution-logo">
        <div class="institution-content">
          <div class="sub-name">SGIT AI Lab, State Grid Corporation of China, 2024.06-2025.07</div>
          <div class="content">Research Intern, mentored by Prof. <a href="https://sallymmx.github.io">Mengmeng Wang</a>, 
            also work with Dr. <a href="https://jingdongwang2017.github.io/">Jingdong Wang</a>.</div>
        </div>
      </div>
    </div>
    
  </td>
</tr>


<!-- Interest Section -->
<tr>
  <td colspan="2" class="section">
    <h2 class="title">Research Interests</h2>
    <div class="subsection">
      <ul>
        <li>
          <p style="font-size: larger;">
  <strong style="font-size: inherit;">Visual Generation with Diffusion Model:</strong> pre-training acceleration, reinforce learning post-training, step distillation, image editing, synthetic dataset.
</p>
        </li>
        <li>
          <p style="font-size: larger;">
            <strong style="font-size: inherit;">Visual Representation and Perception:</strong> self-supervised learning, segmentation, affordance grounding, detection.
          </p>
        </li>
        <li>
          <p style="font-size: larger;">
            <strong style="font-size: inherit;">Unified Modling:</strong> unified multimodal understanding and generation model, vision generalist, the intersection and synergy of representation learning and generative modeling.
          </p>
        </li>
      </ul>
    </div>
  </td>
</tr>



            
 <!-- Project Section -->
  <tr>
  <td colspan="2" class="section">
    <h2 class="title">Projects</h2>


     <div class="subsection">
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tr>
            <td style="width:40%;vertical-align:middle;text-align:center;">  <!-- 添加 margin-right -->
                <img src='images/paper/zimage.jpg' width="285" height="211">
            </td>
            <td style="padding:5px;width:60%;vertical-align:middle">
                <div class="publication-content">
                    <div class="sub-name">
                      Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer
                    </div>
                    <div class="author">
                        Z-Image Team, Alibaba Group
                    </div>
                   
                    <div class="content" style="margin-top:10px;">
                          <a href="https://tongyi-mai.github.io/Z-Image-blog">official site</a>
                          /
                          <a href="https://arxiv.org/abs/2511.22699">paper</a>
                          /
                          <a href="https://github.com/Tongyi-MAI/Z-Image">github page</a>
                          /
                          <a href="https://huggingface.co/Tongyi-MAI/Z-Image-Turbo">huggingface page</a>
                          /
                          <a href="https://huggingface.co/spaces/Tongyi-MAI/Z-Image-Turbo">huggingface space</a>
                          /
                          <a href="https://www.modelscope.cn/models/Tongyi-MAI/Z-Image-Turbo">modelscope page</a>
                          /
                          <a href="https://www.modelscope.cn/aigc/imageGeneration?tab=advanced&versionId=469191&modelType=Checkpoint&sdVersion=Z_IMAGE_TURBO&modelUrl=modelscope%253A%252F%252FTongyi-MAI%252FZ-Image-Turbo%253Frevision%253Dmaster">modelscope space</a>
                          /
                          <a href="https://github.com/Tongyi-MAI/Z-Image/blob/main/assets/Z-Image-Gallery.pdf">offline art gallery</a>
                          /
                          <a href="https://modelscope.cn/studios/Tongyi-MAI/Z-Image-Gallery/summary">online art gallery</a>
                    </div>
                    <div class="content" style="margin-top:10px;">
                   Z-Image, an efficient 6-billion-parameter foundation model for image generation. Through systematic optimization, it proves that top-tier performance is achievable without relying on enormous model sizes, delivering strong results in photorealistic generation and bilingual text rendering that are comparable to leading commercial models.
                </div>
            </td>
        </tr>
    </table>
</div>


    <div class="subsection">
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tr>
            <td style="width:40%;vertical-align:middle;text-align:center;">  <!-- 添加 margin-right -->
                <img src='images/paper/lumina-dimoo.jpg' width="285" height="200">
            </td>
            <td style="padding:5px;width:60%;vertical-align:middle">
                <div class="publication-content">
                    <div class="sub-name">
                      Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding
                    </div>
                    <div class="author">
                        Alpha VLLM Team, Shanghai AI Laboratory
                    </div>
                   
                    <div class="content" style="margin-top:10px;">
                        <a href="https://arxiv.org/abs/2510.06308">technical report</a>
                      /
                        <a href="https://synbol.github.io/Lumina-DiMOO/">project page</a>
                      /
                        <a href="https://github.com/Alpha-VLLM/Lumina-DiMOO">github page</a>  
                       /
                        <a href="https://huggingface.co/Alpha-VLLM/Lumina-DiMOO">huggingface page</a>  
                    </div>
                    <div class="content" style="margin-top:10px;">
                   An open-source foundational model with fully discrete diffusion modeling for seamless multi-modal generation and understanding.
                </div>
            </td>
        </tr>
    </table>
</div>

    
    
  </td>
</tr>

            
            
<!-- Publications Section -->
<tr>
<td colspan="2" class="section">
<h2 class="title">Publications/Preprints (* Co-First Author) <a href="https://scholar.google.com.hk/citations?user=tJcxeMoAAAAJ&hl=zh-CN&oi=ao" style="font-size: 0.8em;">(Google Scholar)</a></h2>



 <div class="subsection">
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tr>
            <td style="width:40%;vertical-align:middle;text-align:center;">  <!-- 添加 margin-right -->
                <img src='images/paper/dmdr.jpg' width="285" height="198">
            </td>
            <td style="padding:5px;width:60%;vertical-align:middle">
                <div class="publication-content">
                    <div class="sub-name">
                      Distribution Matching Distillation Meets Reinforcement Learning
                    </div>
                    <div class="author">
                       <strong><span style="color: purple;">Dengyang Jiang</span></strong>, Dongyang Liu, Zanyi Wang, Qilong Wu, Liuzhuozheng Li, 
                      Hengzhuang Li, Xin Jin, David Liu, Zhen Li, Bo Zhang, Mengmeng Wang, Steven Hoi, Peng Gao, Harry Yang
                    </div>
                  
                    <div class="content" style="color:#666;">
                       <strong><span style="color: red;"><em>Preprint</em>, 2025</span></strong>
                    </div>
                    <div class="content" style="margin-top:10px;">
                        <a href="https://arxiv.org/abs/2511.13649">paper</a>
                      /
                        <a href="https://github.com/vvvvvjdy/dmdr">code</a>            
                    </div>
                  
                    <div class="content" style="margin-top:10px;">
                   Showing that DMD and RL can be trained simultaneously, with RL enabling the student model to surpass the teacher and DMD loss regularizing RL to prevent reward hacking. 

                </div>
            </td>
        </tr>
    </table>
</div>

   <div class="subsection">
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tr>
            <td style="width:40%;vertical-align:middle;text-align:center;">  <!-- 添加 margin-right -->
                <img src='images/paper/think_in_frames.jpg' width="285" height="190">
            </td>
            <td style="padding:5px;width:60%;vertical-align:middle">
                <div class="publication-content">
                    <div class="sub-name">
                      Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning
                    </div>
                    <div class="author">
                       Chengzu Li, Zanyi Wang, Jiaang Li, Yi Xu, Han Zhou, Huanyu Zhang, Ruichuan An, <strong><span style="color: purple;">Dengyang Jiang</span></strong>, Zhaochong An, Ivan Vulić, Serge Belongie, Anna Korhonen
                    </div>
                  
                    <div class="content" style="color:#666;">
                       <strong><span style="color: red;"><em>Preprint</em>, 2026</span></strong>
                    </div>
                    <div class="content" style="margin-top:10px;">
                        <a href="https://arxiv.org/abs/2601.21037">paper</a>  
                      /
                        <a href="https://thinking-in-frames.github.io/">project page</a>
                    </div>
                  
                    <div class="content" style="margin-top:10px;">
                  Demonstrating that video generation models are not merely for visual synthesis but also as powerful engines for visual reasoning.

                </div>
            </td>
        </tr>
    </table>
</div>


   <div class="subsection">
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tr>
            <td style="width:40%;vertical-align:middle;text-align:center;">  <!-- 添加 margin-right -->
                <img src='images/paper/vae-repa.jpg' width="285" height="180">
            </td>
            <td style="padding:5px;width:60%;vertical-align:middle">
                <div class="publication-content">
                    <div class="sub-name">
                      VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training
                    </div>
                    <div class="author">
                      Mengmeng Wang, <strong><span style="color: purple;">Dengyang Jiang</span></strong>, Liuzhuozheng Li, Yucheng Lin, Guojiang Shen, Xiangjie Kong, Yong Liu, Guang Dai, Jingdong Wang
                    </div>
                  
                    <div class="content" style="color:#666;">
                       <strong><span style="color: red;"><em>CVPR</em>, 2026</span></strong>
                    </div>
                    <div class="content" style="margin-top:10px;">
                        <a href="https://arxiv.org/abs/2601.17830">paper</a>
              
                    </div>
                  
                    <div class="content" style="margin-top:10px;">
                   A lightweight framework that leverages off-the-shelf VAE's features to addresses extra forward overhead in the representation alignment for diffusion transformers.

                </div>
            </td>
        </tr>
    </table>
</div>



<div class="subsection">
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tr>
            <td style="width:40%;vertical-align:middle;text-align:center;">  <!-- 添加 margin-right -->
                <img src='images/paper/LaVer.jpg' width="285" height="150">
            </td>
            <td style="padding:5px;width:60%;vertical-align:middle">
                <div class="publication-content">
                    <div class="sub-name">
                     Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models
                    </div>
                    <div class="author">
                       Hengzhuang Li, Xinsong Zhang, Qiming Peng, Bin Luo, Han Hu, 
                       <strong><span style="color: purple;">Dengyang Jiang</span></strong>, Han Jia Ye, Teng Zhang, Hai Jin
                       </div>
                  
                    <div class="content" style="color:#666;">
                       <strong><span style="color: red;"><em>CVPR</em>, 2026</span></strong>
                    </div>
                      
                    <div class="content" style="margin-top:10px;">
                        <a href="https://arxiv.org/abs/2512.06281">paper</a>
                      /
                        <a href="https://github.com/Fir-lat/LaVer">code</a>            
                    </div>
                      
                    <div class="content" style="margin-top:10px;">
                  A novel training framework that facilitates MLLMs in learning more discriminative visual representations via masked image modeling in the joint latent semantic space of LLM. 
                </div>
            </td>
        </tr>
    </table>
</div>

  
  

 <div class="subsection">
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tr>
            <td style="width:40%;vertical-align:middle;text-align:center;">  <!-- 添加 margin-right -->
                <img src='images/paper/EVTAR.jpg' width="285" height="170">
            </td>
            <td style="padding:5px;width:60%;vertical-align:middle">
                <div class="publication-content">
                    <div class="sub-name">
                      RefVTON: person-to-person Try on with Additional Unpaired Visual Reference
                    </div>
                    <div class="author">
                       Liuzhuozheng Li, Yue Gong, Shanyuan Liu, Bo Cheng, Yuhang Ma, Liebucha Wu,  <strong><span style="color: purple;">Dengyang Jiang</span></strong>, Zanyi Wang, Dawei Leng, Yuhui Yin
                    </div>
                    <div class="content" style="color:#666;">
                       <strong><span style="color: red;"><em>CVPR</em>, 2026</span></strong>
                    </div>
                     <div class="content" style="margin-top:10px;">
                        <a href="https://arxiv.org/abs/2511.00956">paper</a>
                        /
                        <a href="https://github.com/360CVGroup/EVTAR">code</a>
                    </div>
                    <div class="content" style="margin-top:10px;">
                      An End-to-End Virtual Try-on model that directly fits the target garment onto the person image.
                </div>
            </td>
        </tr>
    </table>
</div>








  
   <div class="subsection">
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tr>
            <td style="width:40%;vertical-align:middle;text-align:center;">  <!-- 添加 margin-right -->
                <img src='images/paper/SRA.jpg' width="285" height="160">
            </td>
            <td style="padding:5px;width:60%;vertical-align:middle">
                <div class="publication-content">
                    <div class="sub-name">
                      No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves
                    </div>
                    <div class="author">
                       <strong><span style="color: purple;">Dengyang Jiang</span></strong>,
                        Mengmeng Wang, Liuzhuozheng Li, Lei Zhang, Haoyu Wang, Wei Wei, Guang Dai, Yanning Zhang, Jingdong Wang
                    </div>
                    <div class="content" style="color:#666;">
                       <strong><span style="color: red;"><em>ICLR</em>, 2026</span></strong>
                    </div>
                    <div class="content" style="margin-top:10px;">
                        <a href="https://arxiv.org/abs/2505.02831">paper</a>
                      /
                        <a href="https://vvvvvjdy.github.io/sra">project page</a>
                      /
                        <a href="https://github.com/vvvvvjdy/SRA">code</a>            
                    </div>
                    <div class="content" style="margin-top:10px;">
                    Self-representation alignment for enhancing representation learning and generation performance of diffusion transformers.
                </div>
            </td>
        </tr>
    </table>
</div>



  <div class="subsection">
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tr>
            <td style="width:40%;vertical-align:middle;text-align:center;">  <!-- 添加 margin-right -->
                <img src='images/paper/flowrvs.jpg' width="285" height="150">
            </td>
            <td style="padding:5px;width:60%;vertical-align:middle">
                <div class="publication-content">
                    <div class="sub-name">
                      Deforming Videos to Masks: Flow Matching for Referring Video Segmentation
                    </div>
                    <div class="author">
                        Zanyi Wang, <strong><span style="color: purple;">Dengyang Jiang*</span></strong>, Liuzhuozheng Li, Sizhe Dang, Chengzu Li, Harry Yang, Guang Dai, Mengmeng Wang, Jingdong Wang
                    </div>
                    <div class="content" style="color:#666;">
                       <strong><span style="color: red;"><em>ICLR</em>, 2026</span></strong>
                    </div>
                     <div class="content" style="margin-top:10px;">
                        <a href="https://arxiv.org/abs/2510.06139">paper</a>
                        /
                        <a href="https://github.com/xmz111/FlowRVS">code</a>
                    </div>
                    <div class="content" style="margin-top:10px;">
                       Reformulating RVOS as a continuous, text-conditioned flow from video
 to mask and achieving leading performance.
                </div>
            </td>
        </tr>
    </table>
</div>


    <div class="subsection">
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tr>
            <td style="width:40%;vertical-align:middle;text-align:center;">  <!-- 添加 margin-right -->
                <img src='images/paper/mle.jpg' width="285" height="150">
            </td>
            <td style="padding:5px;width:60%;vertical-align:middle">
                <div class="publication-content">
                    <div class="sub-name">
                      AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents
                    </div>
                    <div class="author">
                        Shangheng Du, Xiangchao Yan,  <strong><span style="color: purple;">Dengyang Jiang*</span></strong>, Jiakang Yuan, Yusong Hu, Xin Li, Liang He, Bo Zhang, Lei Bai
                    </div>
                    <div class="content" style="color:#666;">
                       <strong><span style="color: red;"><em>Preprint</em>, 2025</span></strong>
                    </div>
                     <div class="content" style="margin-top:10px;">
                        <a href="https://arxiv.org/abs/2510.08511">paper</a>
                        /
                        <a href="https://github.com/Alpha-Innovator/InternAgent">code</a>
                    </div>
                    <div class="content" style="margin-top:10px;">
                      The LLM-based agent which achieving leading machine learning engineering capability by combining a curated ML knowledge
base with proposed Monte Carlo Graph Search.
                </div>
            </td>
        </tr>
    </table>
</div>


   <div class="subsection">
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tr>
            <td style="width:40%;vertical-align:middle;text-align:center;">  <!-- 添加 margin-right -->
                <img src='images/paper/JoDiffusion.jpg' width="285" height="180">
            </td>
            <td style="padding:5px;width:60%;vertical-align:middle">
                <div class="publication-content">
                    <div class="sub-name">
                     JoDiffusion: Jointly Diffusing Image with Pixel-Level Annotations for Semantic Segmentation Promotion
                    </div>
                    <div class="author">                   
                       Haoyu Wang, Lei Zhang, Wenrui Liu,  <strong><span style="color: purple;">Dengyang Jiang</span></strong>, Wei Wei, Chen Ding
                    </div>
                    <div class="content" style="color:#666;">
                       <strong><span style="color: red;"><em>AAAI</em>, 2026</span></strong>
                    </div>
                    <div class="content" style="margin-top:10px;">
                        <a href="https://arxiv.org/abs/2512.13014">paper</a>     
                       /
                        <a href="https://github.com/00why00/JoDiffusion">code</a>
                    </div>
                    <div class="content" style="margin-top:10px;">
                   A novel framework for semantic segmentation dataset generation promotion by jointly diffusing images with pixel-level annotations.
                </div>
            </td>
        </tr>
    </table>
</div>
  
  
  <div class="subsection">
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tr>
            <td style="width:40%;vertical-align:middle;text-align:center;">  <!-- 添加 margin-right -->
                <img src='images/paper/AffordanceSAM.jpg' width="285" height="180">
            </td>
            <td style="padding:5px;width:60%;vertical-align:middle">
                <div class="publication-content">
                    <div class="sub-name">
                      AffordanceSAM: Segment Anything Once More in Affordance Grounding
                    </div>
                    <div class="author">
                       <strong><span style="color: purple;">Dengyang Jiang</span></strong>,
                        Zanyi Wang, Hengzhuang Li, Sizhe Dang, Teli Ma, Wei Wei, Guang Dai, Lei Zhang, Mengmeng Wang
                    </div>
                    <div class="content" style="color:#666;">
                       <strong><span style="color: red;"><em>Preprint</em>, 2025</span></strong>
                    </div>
                    <div class="content" style="margin-top:10px;">
                        <a href="https://arxiv.org/abs/2504.15650">paper</a> 
                    </div>
                    <div class="content" style="margin-top:10px;">
                    Transferring SAM to affordance grounding task and showing robust performance for both seen and unseen actions.
                </div>
            </td>
        </tr>
    </table>
</div>

  
<div class="subsection">
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tr>
            <td style="width:40%;vertical-align:middle;text-align:center;">  <!-- 添加 margin-right -->
                <img src='images/paper/lbGen.jpg' width="285" height="145">
            </td>
            <td style="padding:5px;width:60%;vertical-align:middle">
                <div class="publication-content">
                    <div class="sub-name">
                      Low-Biased General Annotated Dataset Generation
                    </div>
                    <div class="author">
                       <strong><span style="color: purple;">Dengyang Jiang*</span></strong>,
                        Haoyu Wang,
                        Lei Zhang,
                        Wei Wei,
                        Guang Dai,
                        Mengmeng Wang,
                        Jingdong Wang,
                        Yanning Zhang
                    </div>
                    <div class="content" style="color:#666;">
                       <strong><span style="color: red;"><em>CVPR</em>, 2025</span></strong>
                    </div>
                    <div class="content" style="margin-top:10px;">
                        <a href="https://arxiv.org/abs/2412.10831">paper</a>
                        /
                        <a href="https://github.com/vvvvvjdy/lbGen">code</a>
                    </div>
                    <div class="content" style="margin-top:10px;">
                    A low-biased general annotated dataset (e.g, ImageNet) generation framework helps to obtain more generalized visual backbones.
                </div>
            </td>
        </tr>
    </table>
</div>
  
</td>
</tr>
                        
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:12px;">
                 This web is is adapted from <a href="https://github.com/jonbarron/jonbarron_website" style="font-size:12px;">Jon Barron's website</a>.
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
